package org.submit

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.monotonically_increasing_id

object AutoTable
{

  def main(args: Array[String]): Unit = {

      println("Para poder introducir los argumentos nombrados vamos a mapear el" +
        " array de entrada indicando que nos cree un map separando clave y valor por el " +
        "singno '='. Tras esto, podemos recuperar los values de cada elemento llamando a " +
        "la clave correspondiente.\n")

      val namedArgs = args.map(x => x.split("=")).map(y => (y(0), y(1))).toMap

      println("Iniciando SparkSession ...\n")


      val spark = SparkSession
        .builder()
        .enableHiveSupport()
        .appName("AutoTable")
        .getOrCreate()

      println("En caso de que los argumentos no sean los necesarios lanzamos una excepción " +
        "para finalizar el programa dando detalles de por qué ha ocurrido el fallo." +
        "El resultado de esta excepción también se puede encontrar en el stdout de la " +
        "aplicación.\n")

      if (namedArgs.keys.toSet != Set("database", "table", "location", "regex", "schema", "header"))
      {

        println("Error al introducir los argumentos de la aplicación," +
          " los argumentos permitidos y a su vez obligatorios son:\n")
        println("database (nombre de la base de datos)\n")
        println("table (nombre de la tabla)\n")
        println("location (ruta de los datos de la tabla)\n")
        println("regex (expresión regular para tabular los datos)\n")
        println("schema (esquema de los datos tal y como lo introduciríamos en Hive)\n")
        println("header (utilizar el valor true si queremos indicar que ls primera línea es header" +
          ", cualquier otro valor indica que es falso)\n")
        println(" La sintaxis para introducir estos argumentos es: argumento=valor\n")

        throw new IllegalArgumentException("Los argumentos introducidos son incorrectos. \n\n" +
          "Los argumentos permitidos y a su vez obligatorios son: \n\n" +
          "database (nombre de la base de datos)\n\n" +
          "table (nombre de la tabla)\n\n" +
          "location (ruta de los datos de la tabla)\n\n" +
          "regex (expresión regular para tabular los datos)\n\n" +
          "schema (esquema de los datos tal y como lo introduciríamos en Hive)\n\n" +
          "header (utilizar el valor true si queremos indicar que ls primera línea es header," +
          " cualquier otro valor indica que es falso)")

      }

      spark.sql("""create database if not exists %s""".format(namedArgs("database")))

      spark.sql("USE %s;".format(namedArgs("database")))

      println("Creando tabla de Hive con los argumentos: \n")

      println("database = %s\n".format(namedArgs("database")))

      println("table = %s\n".format(namedArgs("table")))

      println("schema = %s\n".format(namedArgs("schema")))

      println("regex = %s\n".format(namedArgs("regex").replaceAll("""\\""", """\\""")))

      println("location = %s\n".format(namedArgs("location")))

      println("header = %s\n".format(namedArgs("header")))



      spark.sql(
        """CREATE external TABLE %s (%s)
          |ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
          |WITH SERDEPROPERTIES ('input.regex'='%s')
          |location '%s';""".stripMargin.format(namedArgs("table"),
          namedArgs("schema"),
          namedArgs("regex").replaceAll("""\\""", """\\\\"""),
          namedArgs("location")))


      println("La query lanzada para crear la tabla ha sido:\n")



      println(
        """CREATE external TABLE %s (%s)
          |ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
          |WITH SERDEPROPERTIES ('input.regex'='%s')
          |location '%s';""".stripMargin.format(namedArgs("table"),
          namedArgs("schema"),
          namedArgs("regex").replaceAll("""\\""", """\\\\"""),
          namedArgs("location")))

      println()

      println("Como no funcionan las 'tableproperties' en la tabla y los headers no encajan con el formato de" +
        " la RegEx, tenemos que eliminar manualmente la primera fila de la tabla." +
        " Haremos esto sólo en caso de que el argumento header tenga el valor 'true'\n")

      println("Debemos tener cuidado con esto ya que normalmente los datos en Spark" +
        " tienden a ser particionados y no podemos eliminar el primer elemento de" +
        " cualquier partición. Por ello utilizamos la función 'monotonically_increasing_id()'" +
        " para crear una columna con los índices numéricos. Mientras tengamos menos de" +
        " varios miles de particiones este método debería funcionar como se espera.\n")

      if (namedArgs("header") == "true") {


        println("Se ha seleccionado la opción de eliminar el header.\n")


        val dataux = spark.sql("""SELECT * FROM %s""".format(namedArgs("table")))

        println("Podemos mostrar cómo queda el DataFrame cuando le asignamos la " +
          "columna con la id incremental.\n")

        dataux.select("*")
          .withColumn("number", monotonically_increasing_id())
          .show(25)

        val dataux2 = dataux.select("*")
          .withColumn("number", monotonically_increasing_id())
          .where("number != 0")
          .drop("number")


        println("Podemos mostrar el número de particiones del DataFrame bajando " +
          "al nivel de RDD." +dataux2.rdd.getNumPartitions)
        println()
        println("Es interesante ya que sabemos que el método utilizado " +
        "para asignar ids funciona mal con más de 1 billón de particiones.")


        dataux2.createOrReplaceTempView("tbaux")

        spark.sql("""DROP TABLE IF EXISTS %s""".format(namedArgs("table")))

        spark.sql("""CREATE TABLE %s AS (SELECT * FROM tbaux)""".format(namedArgs("table")))

      }


      println("Mostramos las primeras 25 líneas para ver el resultado de la tabulación:\n")


      val dataset = spark.sql("""select * from %s""".format(namedArgs("table")))

      dataset.show(25)

      println("El número de filas del dataset es: " + dataset.count())
      println()

      println("Tipos de los datos:\n\n" + dataset.dtypes.mkString("Array(", ", ", ")"))

    }

}
